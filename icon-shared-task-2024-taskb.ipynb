{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9955050,"sourceType":"datasetVersion","datasetId":6122591}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-20T10:41:26.062865Z","iopub.execute_input":"2024-11-20T10:41:26.063217Z","iopub.status.idle":"2024-11-20T10:41:27.009305Z","shell.execute_reply.started":"2024-11-20T10:41:26.063184Z","shell.execute_reply":"2024-11-20T10:41:27.008483Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/Val_Task_A.csv\n/kaggle/input/Train_Task_B.csv\n/kaggle/input/Train_Task_A.csv\n/kaggle/input/Test_Task_A.csv\n/kaggle/input/Test_Task_B.csv\n/kaggle/input/Val_Task_B.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# !pip install datasets transformers[sentencepiece]\n# !pip install evaluate\n# !pip install accelerate==0.26.0\n# !pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-11-20T10:41:27.010801Z","iopub.execute_input":"2024-11-20T10:41:27.011158Z","iopub.status.idle":"2024-11-20T10:41:27.014538Z","shell.execute_reply.started":"2024-11-20T10:41:27.011130Z","shell.execute_reply":"2024-11-20T10:41:27.013678Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport json\nimport math\nimport re\n\ndef clean(text):\n    # Convert to lowercase and strip spaces\n    text = text.lower().strip()\n    # Replace special characters with a space\n    text = re.sub(r\"[^a-zA-Z0-9\\s]\", '', text)\n    return text\n\ndef convert(file_path, output_file_path):\n    # Read the CSV file\n    df = pd.read_csv(file_path)\n\n    # Define mappings for 'Target' and 'Severity'\n    target_mapping = {'I': 1, 'O': 2, 'R': 3}\n    severity_mapping = {'L': 1, 'M': 2, 'H': 3}\n\n    # Convert the DataFrame to a list of dictionaries, skipping invalid rows\n    data = []\n    for _, row in df.iterrows():\n        target = target_mapping.get(row[\"Target\"])\n        severity = severity_mapping.get(row[\"Severity\"])\n        if target is None:\n            target = 0\n        if severity is None:\n            severity = 0\n        data.append({\n            \"text\": clean(str(row[\"Tweet\"])),\n            \"label1\": target,\n            \"label2\": severity\n        })\n\n    # Save the list of dictionaries as a JSON file\n    with open(output_file_path, 'w') as f:\n        json.dump(data, f, indent=4)\n\n    print(f\"Data has been saved to {output_file_path}\")\n\nconvert('/kaggle/input/Train_Task_B.csv', '/kaggle/working/train.json')\nconvert('/kaggle/input/Val_Task_B.csv', '/kaggle/working/val.json')","metadata":{"execution":{"iopub.status.busy":"2024-11-20T10:41:27.015568Z","iopub.execute_input":"2024-11-20T10:41:27.015855Z","iopub.status.idle":"2024-11-20T10:41:27.505749Z","shell.execute_reply.started":"2024-11-20T10:41:27.015827Z","shell.execute_reply":"2024-11-20T10:41:27.504843Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Data has been saved to /kaggle/working/train.json\nData has been saved to /kaggle/working/val.json\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport torch\nimport torch.nn as nn\nfrom datasets import load_dataset\nfrom transformers import DataCollatorWithPadding\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nfrom transformers.modeling_outputs import SequenceClassifierOutput\n\nclass RobertaWithTwoHeads(nn.Module):\n    def __init__(self, base_model, num_labels_head1=4, num_labels_head2=4, hidden_size=768):\n        super(RobertaWithTwoHeads, self).__init__()\n        self.roberta = base_model.roberta\n        self.dropout = nn.Dropout(base_model.config.hidden_dropout_prob)\n        self.config = base_model.config\n        \n        # Enhanced classification head 1 for hate detection\n        self.head1 = nn.Sequential(\n            nn.Linear(self.config.hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.GELU(),\n            nn.Dropout(0.2),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.LayerNorm(hidden_size // 2),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size // 2, num_labels_head1)\n        )\n        \n        # Enhanced classification head 2 for fake news detection\n        self.head2 = nn.Sequential(\n            nn.Linear(self.config.hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.GELU(),\n            nn.Dropout(0.2),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.LayerNorm(hidden_size // 2),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size // 2, num_labels_head2)\n        )\n        \n        # Optional: Add residual connections\n        self.use_residual = True\n        if self.use_residual:\n            self.residual_projection1 = nn.Linear(self.config.hidden_size, num_labels_head1)\n            self.residual_projection2 = nn.Linear(self.config.hidden_size, num_labels_head2)\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        labels=None,\n        return_dict=None,\n        **kwargs\n    ):\n        outputs = self.roberta(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            return_dict=True\n        )\n\n        sequence_output = outputs.last_hidden_state\n        pooled_output = sequence_output[:, 0, :]  # [CLS] token\n        pooled_output = self.dropout(pooled_output)\n\n        # Forward pass through enhanced heads\n        head1_output = self.head1(pooled_output)\n        head2_output = self.head2(pooled_output)\n\n        # Add residual connections if enabled\n        if self.use_residual:\n            residual1 = self.residual_projection1(pooled_output)\n            residual2 = self.residual_projection2(pooled_output)\n            logits_head1 = head1_output + residual1\n            logits_head2 = head2_output + residual2\n        else:\n            logits_head1 = head1_output\n            logits_head2 = head2_output\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            if labels.dim() == 2:\n                loss_head1 = loss_fct(logits_head1, labels[:, 0])\n                loss_head2 = loss_fct(logits_head2, labels[:, 1])\n                loss = (loss_head1 + loss_head2) / 2\n            else:\n                print(f\"Unexpected label shape: {labels.shape}\")\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=(logits_head1, logits_head2),\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\ndef tokenize_function(examples):\n    # Tokenize the texts\n    tokenized = tokenizer(\n        examples[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=512\n    )\n    # Process labels correctly - don't duplicate\n    labels = torch.tensor([\n        [examples['label1'][i], examples['label2'][i]]\n        for i in range(len(examples['text']))\n    ], dtype=torch.long)\n    \n    tokenized['labels'] = labels\n    return tokenized\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    logits_head1, logits_head2 = logits\n    \n    predictions_head1 = np.argmax(logits_head1, axis=1)\n    predictions_head2 = np.argmax(logits_head2, axis=1)\n    \n    # Ensure labels are properly shaped\n    if labels.ndim == 2:\n        labels_head1, labels_head2 = labels[:, 0], labels[:, 1]\n    else:\n        raise ValueError(f\"Unexpected label shape: {labels.shape}\")\n    \n    accuracy_head1 = accuracy_score(labels_head1, predictions_head1)\n    accuracy_head2 = accuracy_score(labels_head2, predictions_head2)\n    \n    return {\n        \"accuracy_head1\": accuracy_head1,\n        \"accuracy_head2\": accuracy_head2,\n        \"overall_accuracy\": (accuracy_head1 + accuracy_head2) / 2,\n    }\n\n# Model and tokenizer initialization\nmodel_checkpoint = \"roberta-base\"\nbatch_size = 8\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\n# Load dataset\ndata_files = {\n    \"train\": \"/kaggle/working/train.json\",\n    \"validation\": \"/kaggle/working/val.json\"\n}\ndataset = load_dataset(\"json\", data_files=data_files)\n\n# Tokenize datasets\ntokenized_dataset = dataset.map(\n    tokenize_function,\n    batched=True,\n)\n\n# Load base model and create the dual-head model\nbase_model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\nmodel = RobertaWithTwoHeads(base_model)\n\n# Training arguments optimized for full fine-tuning\ntraining_args = TrainingArguments(\n    output_dir=\"roberta-dual-head\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=6,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"overall_accuracy\",\n    greater_is_better=True,\n    report_to=[\"none\"],\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    fp16=True,  # Mixed precision training\n    gradient_accumulation_steps=4,\n    warmup_ratio=0.1,  # Added warmup\n)\n\n# Initialize trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n    compute_metrics=compute_metrics,\n)\n\n# Train the model\ntrainer.train()\n\n# Save the model\ntrainer.save_model(\"roberta-dual-head-finetuned\")","metadata":{"execution":{"iopub.status.busy":"2024-11-20T10:41:27.507230Z","iopub.execute_input":"2024-11-20T10:41:27.507605Z","iopub.status.idle":"2024-11-20T11:16:00.194227Z","shell.execute_reply.started":"2024-11-20T10:41:27.507563Z","shell.execute_reply":"2024-11-20T11:16:00.193170Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e99ae01d0ad045b2bfdcae4b0f5c41ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30ca1450c5974dcaa2c7706bc5ea48ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8b9f1c4cf0b4efc943b88e2612c7246"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"201f6e6e1408417e84908021707fae83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a87f5faebd3b434b98f15228fc00ddd6"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d2cda994ebc48219bdc7dcdebaff39b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3519a30eb12c45aea5532ae9f3f3e1b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6396 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8103c898b644005adc734739e1cc4c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"459ede8d40834ef0aa398cc67cf50abe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1735bf28c09a46119a76d796e686b4e5"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1200/1200 34:01, Epoch 6/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy Head1</th>\n      <th>Accuracy Head2</th>\n      <th>Overall Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.099600</td>\n      <td>1.032722</td>\n      <td>0.638750</td>\n      <td>0.541250</td>\n      <td>0.590000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.961600</td>\n      <td>0.965756</td>\n      <td>0.653750</td>\n      <td>0.561250</td>\n      <td>0.607500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.898700</td>\n      <td>1.009790</td>\n      <td>0.662500</td>\n      <td>0.567500</td>\n      <td>0.615000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.832100</td>\n      <td>0.996495</td>\n      <td>0.662500</td>\n      <td>0.578750</td>\n      <td>0.620625</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.777200</td>\n      <td>1.022386</td>\n      <td>0.660000</td>\n      <td>0.576250</td>\n      <td>0.618125</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.732200</td>\n      <td>1.040034</td>\n      <td>0.676250</td>\n      <td>0.577500</td>\n      <td>0.626875</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n# import torch\n# import torch.nn as nn\n# from transformers.modeling_outputs import SequenceClassifierOutput\n# from safetensors.torch import load_file\n\n# class RobertaWithTwoHeads(nn.Module):\n#     def __init__(self, base_model, num_labels_head1=4, num_labels_head2=4, hidden_size=768):\n#         super(RobertaWithTwoHeads, self).__init__()\n#         self.roberta = base_model.roberta\n#         self.dropout = nn.Dropout(base_model.config.hidden_dropout_prob)\n#         self.config = base_model.config\n        \n#         # Enhanced classification head 1 for hate detection\n#         self.head1 = nn.Sequential(\n#             nn.Linear(self.config.hidden_size, hidden_size),\n#             nn.LayerNorm(hidden_size),\n#             nn.GELU(),\n#             nn.Dropout(0.2),\n#             nn.Linear(hidden_size, hidden_size // 2),\n#             nn.LayerNorm(hidden_size // 2),\n#             nn.GELU(),\n#             nn.Dropout(0.1),\n#             nn.Linear(hidden_size // 2, num_labels_head1)\n#         )\n        \n#         # Enhanced classification head 2 for fake news detection\n#         self.head2 = nn.Sequential(\n#             nn.Linear(self.config.hidden_size, hidden_size),\n#             nn.LayerNorm(hidden_size),\n#             nn.GELU(),\n#             nn.Dropout(0.2),\n#             nn.Linear(hidden_size, hidden_size // 2),\n#             nn.LayerNorm(hidden_size // 2),\n#             nn.GELU(),\n#             nn.Dropout(0.1),\n#             nn.Linear(hidden_size // 2, num_labels_head2)\n#         )\n        \n#         # Optional: Add residual connections\n#         self.use_residual = True\n#         if self.use_residual:\n#             self.residual_projection1 = nn.Linear(self.config.hidden_size, num_labels_head1)\n#             self.residual_projection2 = nn.Linear(self.config.hidden_size, num_labels_head2)\n\n#     def forward(\n#         self,\n#         input_ids=None,\n#         attention_mask=None,\n#         labels=None,\n#         return_dict=None,\n#         **kwargs\n#     ):\n#         outputs = self.roberta(\n#             input_ids=input_ids,\n#             attention_mask=attention_mask,\n#             return_dict=True\n#         )\n\n#         sequence_output = outputs.last_hidden_state\n#         pooled_output = sequence_output[:, 0, :]  # [CLS] token\n#         pooled_output = self.dropout(pooled_output)\n\n#         # Forward pass through enhanced heads\n#         head1_output = self.head1(pooled_output)\n#         head2_output = self.head2(pooled_output)\n\n#         # Add residual connections if enabled\n#         if self.use_residual:\n#             residual1 = self.residual_projection1(pooled_output)\n#             residual2 = self.residual_projection2(pooled_output)\n#             logits_head1 = head1_output + residual1\n#             logits_head2 = head2_output + residual2\n#         else:\n#             logits_head1 = head1_output\n#             logits_head2 = head2_output\n\n#         loss = None\n#         if labels is not None:\n#             loss_fct = nn.CrossEntropyLoss()\n#             if labels.dim() == 2:\n#                 loss_head1 = loss_fct(logits_head1, labels[:, 0])\n#                 loss_head2 = loss_fct(logits_head2, labels[:, 1])\n#                 loss = (loss_head1 + loss_head2) / 2\n#             else:\n#                 print(f\"Unexpected label shape: {labels.shape}\")\n\n#         return SequenceClassifierOutput(\n#             loss=loss,\n#             logits=(logits_head1, logits_head2),\n#             hidden_states=outputs.hidden_states,\n#             attentions=outputs.attentions,\n#         )\n\n\n# # Load tokenizer\n# tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n\n# def clean(text):\n#     # Add your text cleaning function here\n#     # This is a placeholder - use your actual cleaning logic\n#     return str(text).strip()\n\n# # Load base model and create the dual-head model\n# base_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n# model = RobertaWithTwoHeads(base_model)\n\n# # Load the fine-tuned weights from safetensors format\n# state_dict = load_file(\"roberta-dual-head-finetuned/model.safetensors\")\n# model.load_state_dict(state_dict)\n# model.eval()  # Set to evaluation mode\n\n# # Inference loop\n# df = pd.read_csv(\"/kaggle/input/Val_Task_B.csv\")\n# correct_hate = 0\n# correct_fake = 0\n# total = 0\n\n# # If GPU is available\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model = model.to(device)\n\n# target_mapping = {'I': 0, 'O': 1, 'R': 2}\n# severity_mapping = {'L': 0, 'M': 1, 'H': 2}\n# target_reverse_mapping = {0: 'I', 1: 'O', 2: 'R'}\n# severity_reverse_mapping = {0: 'L', 1: 'M', 2: 'H'}\n\n# for _, row in df.iterrows():\n#     if(row[\"Target\"] not in [\"I\",\"O\",\"R\"] or row[\"Severity\"] not in [\"L\", \"M\", \"H\"]):\n#         continue\n#     text = clean(str(row[\"Tweet\"]))\n#     inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n    \n#     # Move inputs to the same device as model\n#     inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n#     # Perform inference\n#     with torch.no_grad():\n#         outputs = model(**inputs)\n        \n#     # Extract predictions\n#     logits_head1, logits_head2 = outputs.logits\n    \n#     # Get predicted classes\n#     pred_head1 = torch.argmax(logits_head1, dim=1)\n#     pred_head2 = torch.argmax(logits_head2, dim=1)\n    \n#     # Move predictions to CPU for comparison with pandas data\n#     pred_head1 = pred_head1.cpu()\n#     pred_head2 = pred_head2.cpu()\n    \n#     # Get ground truth labels\n#     true_hate = int(target_mapping[row[\"Target\"]])\n#     true_fake = int(severity_mapping[row[\"Severity\"]])\n    \n#     # Update counters\n#     if pred_head1.item() == true_hate:\n#         correct_hate += 1\n#     if pred_head2.item() == true_fake:\n#         correct_fake += 1\n    \n#     total += 1\n\n# # Calculate accuracies\n# hate_accuracy = correct_hate / total\n# fake_accuracy = correct_fake / total\n# overall_accuracy = (hate_accuracy + fake_accuracy) / 2\n\n# print(f\"Hate Detection Accuracy: {hate_accuracy:.4f}\")\n# print(f\"Fake News Detection Accuracy: {fake_accuracy:.4f}\")\n# print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n\n# # Optional: Save predictions to CSV\n# predictions = []\n# model.eval()\n# with torch.no_grad():\n#     for _, row in df.iterrows():\n#         if(row[\"Target\"] not in [\"I\",\"O\",\"R\"] or row[\"Severity\"] not in [\"L\", \"M\", \"H\"]):\n#             continue\n#         text = clean(str(row[\"Tweet\"]))\n#         inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n#         inputs = {k: v.to(device) for k, v in inputs.items()}\n        \n#         outputs = model(**inputs)\n#         logits_head1, logits_head2 = outputs.logits\n        \n#         pred_hate = torch.argmax(logits_head1, dim=1).cpu().item()\n#         pred_fake = torch.argmax(logits_head2, dim=1).cpu().item()\n        \n#         predictions.append({\n#             'Tweet': row[\"Tweet\"],\n#             'Predicted_Target': target_reverse_mapping[pred_hate],\n#             'Predicted_Severity': severity_reverse_mapping[pred_fake],\n#             'True_Target': row[\"Target\"],\n#             'True_Severity': row[\"Severity\"]\n#         })\n\n# predictions_df = pd.DataFrame(predictions)\n# predictions_df.to_csv(\"predictions.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-20T11:16:00.196390Z","iopub.execute_input":"2024-11-20T11:16:00.196671Z","iopub.status.idle":"2024-11-20T11:16:00.204633Z","shell.execute_reply.started":"2024-11-20T11:16:00.196645Z","shell.execute_reply":"2024-11-20T11:16:00.203850Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nimport torch.nn as nn\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nfrom safetensors.torch import load_file\n\nclass RobertaWithTwoHeads(nn.Module):\n    def __init__(self, base_model, num_labels_head1=4, num_labels_head2=4, hidden_size=768):\n        super(RobertaWithTwoHeads, self).__init__()\n        self.roberta = base_model.roberta\n        self.dropout = nn.Dropout(base_model.config.hidden_dropout_prob)\n        self.config = base_model.config\n        \n        # Enhanced classification head 1 for hate detection\n        self.head1 = nn.Sequential(\n            nn.Linear(self.config.hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.GELU(),\n            nn.Dropout(0.2),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.LayerNorm(hidden_size // 2),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size // 2, num_labels_head1)\n        )\n        \n        # Enhanced classification head 2 for fake news detection\n        self.head2 = nn.Sequential(\n            nn.Linear(self.config.hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.GELU(),\n            nn.Dropout(0.2),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.LayerNorm(hidden_size // 2),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size // 2, num_labels_head2)\n        )\n        \n        # Optional: Add residual connections\n        self.use_residual = True\n        if self.use_residual:\n            self.residual_projection1 = nn.Linear(self.config.hidden_size, num_labels_head1)\n            self.residual_projection2 = nn.Linear(self.config.hidden_size, num_labels_head2)\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        labels=None,\n        return_dict=None,\n        **kwargs\n    ):\n        outputs = self.roberta(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            return_dict=True\n        )\n\n        sequence_output = outputs.last_hidden_state\n        pooled_output = sequence_output[:, 0, :]  # [CLS] token\n        pooled_output = self.dropout(pooled_output)\n\n        # Forward pass through enhanced heads\n        head1_output = self.head1(pooled_output)\n        head2_output = self.head2(pooled_output)\n\n        # Add residual connections if enabled\n        if self.use_residual:\n            residual1 = self.residual_projection1(pooled_output)\n            residual2 = self.residual_projection2(pooled_output)\n            logits_head1 = head1_output + residual1\n            logits_head2 = head2_output + residual2\n        else:\n            logits_head1 = head1_output\n            logits_head2 = head2_output\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            if labels.dim() == 2:\n                loss_head1 = loss_fct(logits_head1, labels[:, 0])\n                loss_head2 = loss_fct(logits_head2, labels[:, 1])\n                loss = (loss_head1 + loss_head2) / 2\n            else:\n                print(f\"Unexpected label shape: {labels.shape}\")\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=(logits_head1, logits_head2),\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n\ndef clean(text):\n    # Add your text cleaning function here\n    # This is a placeholder - use your actual cleaning logic\n    return str(text).strip()\n\n# Load base model and create the dual-head model\nbase_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=3)\nmodel = RobertaWithTwoHeads(base_model)\n\n# Load the fine-tuned weights from safetensors format\nstate_dict = load_file(\"roberta-dual-head-finetuned/model.safetensors\")\nmodel.load_state_dict(state_dict)\nmodel.eval()  # Set to evaluation mode\n\ntarget_reverse_mapping = {0: \"N/A\", 1: 'I', 2: 'O', 3: 'R'}\nseverity_reverse_mapping = {0: \"N/A\", 1: 'L', 2: 'M', 3: 'H'}\n\n# Inference loop\ndf = pd.read_csv(\"/kaggle/input/Test_Task_B.csv\")\n\n# If GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\npredictions = []\nmodel.eval()\nwith torch.no_grad():\n    for _, row in df.iterrows():\n        text = clean(str(row[\"Tweet\"]))\n        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        \n        outputs = model(**inputs)\n        logits_head1, logits_head2 = outputs.logits\n        \n        pred_hate = torch.argmax(logits_head1, dim=1).cpu().item()\n        pred_fake = torch.argmax(logits_head2, dim=1).cpu().item()\n        \n        predictions.append({\n            'Id': row[\"Id\"],\n            'Tweet': row[\"Tweet\"],\n            'Target': target_reverse_mapping[pred_hate],\n            'Severity': severity_reverse_mapping[pred_fake]\n        })\n\npredictions_df = pd.DataFrame(predictions)\npredictions_df.to_csv(\"KeyboardWarriors_TaskB_run1.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-20T11:16:00.206023Z","iopub.execute_input":"2024-11-20T11:16:00.206363Z","iopub.status.idle":"2024-11-20T11:16:09.405335Z","shell.execute_reply.started":"2024-11-20T11:16:00.206322Z","shell.execute_reply":"2024-11-20T11:16:09.404235Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"print(\"Done\")","metadata":{"execution":{"iopub.status.busy":"2024-11-20T11:16:09.406440Z","iopub.execute_input":"2024-11-20T11:16:09.406689Z","iopub.status.idle":"2024-11-20T11:16:09.410929Z","shell.execute_reply.started":"2024-11-20T11:16:09.406664Z","shell.execute_reply":"2024-11-20T11:16:09.410118Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Done\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}