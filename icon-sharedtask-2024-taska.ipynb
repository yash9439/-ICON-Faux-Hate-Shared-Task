{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9955050,"sourceType":"datasetVersion","datasetId":6122591}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-20T08:39:10.270559Z","iopub.execute_input":"2024-11-20T08:39:10.270924Z","iopub.status.idle":"2024-11-20T08:39:10.607064Z","shell.execute_reply.started":"2024-11-20T08:39:10.270895Z","shell.execute_reply":"2024-11-20T08:39:10.606180Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/icon-2024-sharedtask/Val_Task_A.csv\n/kaggle/input/icon-2024-sharedtask/Train_Task_B.csv\n/kaggle/input/icon-2024-sharedtask/Train_Task_A.csv\n/kaggle/input/icon-2024-sharedtask/Test_Task_A.csv\n/kaggle/input/icon-2024-sharedtask/Test_Task_B.csv\n/kaggle/input/icon-2024-sharedtask/Val_Task_B.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# !pip install datasets transformers[sentencepiece]\n# !pip install evaluate\n# !pip install accelerate==0.26.0\n# !pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-11-20T08:39:10.609043Z","iopub.execute_input":"2024-11-20T08:39:10.609544Z","iopub.status.idle":"2024-11-20T08:39:10.613497Z","shell.execute_reply.started":"2024-11-20T08:39:10.609480Z","shell.execute_reply":"2024-11-20T08:39:10.612572Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport json\nimport math\nimport re\n\ndef clean(text):\n    # Convert to lowercase and strip spaces\n    text = text.lower().strip()\n    # Replace special characters with a space\n    text = re.sub(r\"[^a-zA-Z0-9\\s]\", '', text)\n    return text\n\ndef convert(file_path, output_file_path):\n    # Read the CSV file\n    df = pd.read_csv(file_path)\n\n    # Convert the DataFrame to a list of dictionaries, skipping invalid rows\n    data = []\n    for _, row in df.iterrows():\n        # Check if 'Hate' or 'Fake' is not a number, None, or NaN\n        if (\n            isinstance(row[\"Hate\"], (int, float)) and not math.isnan(row[\"Hate\"]) and\n            isinstance(row[\"Fake\"], (int, float)) and not math.isnan(row[\"Fake\"])\n        ):\n            data.append({\n                \"text\": clean(str(row[\"Tweet\"])),\n                \"label1\": int(row[\"Hate\"]),  # Convert to int for consistency\n                \"label2\": int(row[\"Fake\"])\n            })\n\n    # Save the list of dictionaries as a JSON file\n    with open(output_file_path, 'w') as f:\n        json.dump(data, f, indent=4)\n\n    print(f\"Data has been saved to {output_file_path}\")\n    \ndef convert_test(file_path, output_file_path):\n    # Read the CSV file\n    df = pd.read_csv(file_path)\n\n    # Convert the DataFrame to a list of dictionaries\n    data = [\n        {\"text\": clean(str(row[\"Tweet\"]))}\n        for _, row in df.iterrows()\n    ]\n\n    # Save the list of dictionaries as a JSON file\n    with open(output_file_path, 'w') as f:\n        json.dump(data, f, indent=4)\n\n    print(f\"Data has been saved to {output_file_path}\")\n\nconvert('/kaggle/input/icon-2024-sharedtask/Train_Task_A.csv', '/kaggle/working/train.json')\nconvert('/kaggle/input/icon-2024-sharedtask/Val_Task_A.csv', '/kaggle/working/val.json')\nconvert_test('/kaggle/input/icon-2024-sharedtask/Test_Task_A.csv', '/kaggle/working/test.json')","metadata":{"execution":{"iopub.status.busy":"2024-11-20T08:39:10.614866Z","iopub.execute_input":"2024-11-20T08:39:10.615188Z","iopub.status.idle":"2024-11-20T08:39:11.224660Z","shell.execute_reply.started":"2024-11-20T08:39:10.615153Z","shell.execute_reply":"2024-11-20T08:39:11.223596Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Data has been saved to /kaggle/working/train.json\nData has been saved to /kaggle/working/val.json\nData has been saved to /kaggle/working/test.json\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport torch\nimport torch.nn as nn\nfrom datasets import load_dataset\nfrom transformers import DataCollatorWithPadding\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nfrom transformers.modeling_outputs import SequenceClassifierOutput\n\nclass RobertaWithTwoHeads(nn.Module):\n    def __init__(self, base_model, num_labels_head1=2, num_labels_head2=2, hidden_size=768):\n        super(RobertaWithTwoHeads, self).__init__()\n        self.roberta = base_model.roberta\n        self.dropout = nn.Dropout(base_model.config.hidden_dropout_prob)\n        self.config = base_model.config\n        \n        # Enhanced classification head 1 for hate detection\n        self.head1 = nn.Sequential(\n            nn.Linear(self.config.hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.GELU(),\n            nn.Dropout(0.2),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.LayerNorm(hidden_size // 2),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size // 2, num_labels_head1)\n        )\n        \n        # Enhanced classification head 2 for fake news detection\n        self.head2 = nn.Sequential(\n            nn.Linear(self.config.hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.GELU(),\n            nn.Dropout(0.2),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.LayerNorm(hidden_size // 2),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size // 2, num_labels_head2)\n        )\n        \n        # Optional: Add residual connections\n        self.use_residual = False\n        if self.use_residual:\n            self.residual_projection1 = nn.Linear(self.config.hidden_size, num_labels_head1)\n            self.residual_projection2 = nn.Linear(self.config.hidden_size, num_labels_head2)\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        labels=None,\n        return_dict=None,\n        **kwargs\n    ):\n        outputs = self.roberta(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            return_dict=True\n        )\n\n        sequence_output = outputs.last_hidden_state\n        pooled_output = sequence_output[:, 0, :]  # [CLS] token\n        pooled_output = self.dropout(pooled_output)\n\n        # Forward pass through enhanced heads\n        head1_output = self.head1(pooled_output)\n        head2_output = self.head2(pooled_output)\n\n        # Add residual connections if enabled\n        if self.use_residual:\n            residual1 = self.residual_projection1(pooled_output)\n            residual2 = self.residual_projection2(pooled_output)\n            logits_head1 = head1_output + residual1\n            logits_head2 = head2_output + residual2\n        else:\n            logits_head1 = head1_output\n            logits_head2 = head2_output\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            if labels.dim() == 2:\n                loss_head1 = loss_fct(logits_head1, labels[:, 0])\n                loss_head2 = loss_fct(logits_head2, labels[:, 1])\n                loss = (loss_head1 + loss_head2) / 2\n            else:\n                print(f\"Unexpected label shape: {labels.shape}\")\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=(logits_head1, logits_head2),\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\ndef tokenize_function(examples):\n    # Tokenize the texts\n    tokenized = tokenizer(\n        examples[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=512\n    )\n    # Process labels correctly - don't duplicate\n    labels = torch.tensor([\n        [examples['label1'][i], examples['label2'][i]]\n        for i in range(len(examples['text']))\n    ], dtype=torch.long)\n    \n    tokenized['labels'] = labels\n    return tokenized\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    logits_head1, logits_head2 = logits\n    \n    predictions_head1 = np.argmax(logits_head1, axis=1)\n    predictions_head2 = np.argmax(logits_head2, axis=1)\n    \n    # Ensure labels are properly shaped\n    if labels.ndim == 2:\n        labels_head1, labels_head2 = labels[:, 0], labels[:, 1]\n    else:\n        raise ValueError(f\"Unexpected label shape: {labels.shape}\")\n    \n    accuracy_head1 = accuracy_score(labels_head1, predictions_head1)\n    accuracy_head2 = accuracy_score(labels_head2, predictions_head2)\n    \n    return {\n        \"accuracy_head1\": accuracy_head1,\n        \"accuracy_head2\": accuracy_head2,\n        \"overall_accuracy\": (accuracy_head1 + accuracy_head2) / 2,\n    }\n\n# Model and tokenizer initialization\nmodel_checkpoint = \"roberta-base\"\nbatch_size = 8\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\n# Load dataset\ndata_files = {\n    \"train\": \"/kaggle/working/train.json\",\n    \"validation\": \"/kaggle/working/val.json\"\n}\ndataset = load_dataset(\"json\", data_files=data_files)\n\n# Tokenize datasets\ntokenized_dataset = dataset.map(\n    tokenize_function,\n    batched=True,\n)\n\n# Load base model and create the dual-head model\nbase_model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\nmodel = RobertaWithTwoHeads(base_model)\n\n# Training arguments optimized for full fine-tuning\ntraining_args = TrainingArguments(\n    output_dir=\"roberta-dual-head\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"overall_accuracy\",\n    greater_is_better=True,\n    report_to=[\"none\"],\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    fp16=True,  # Mixed precision training\n    gradient_accumulation_steps=4,\n    warmup_ratio=0.1,  # Added warmup\n)\n\n# Initialize trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n    compute_metrics=compute_metrics,\n)\n\n# Train the model\ntrainer.train()\n\n# Save the model\ntrainer.save_model(\"roberta-dual-head-finetuned\")","metadata":{"execution":{"iopub.status.busy":"2024-11-20T08:39:11.226043Z","iopub.execute_input":"2024-11-20T08:39:11.226411Z","iopub.status.idle":"2024-11-20T09:08:02.136228Z","shell.execute_reply.started":"2024-11-20T08:39:11.226373Z","shell.execute_reply":"2024-11-20T09:08:02.135312Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f531ea9eb034fb5a7419b02f0877f81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f26eefe02fb1482f8c86f767548da70f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e41c62af748d48829d12eea00dd1e6c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4b3df9efbcc47b8ac2d6555e63a4633"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b795b9ffba347eb86ddade3c8887f76"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c6037c28d7d4cf7b51f2fb27463e8da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6da0015815e4232be614f69625f9799"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6396 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af691aeeec98463cbf1c4431b22a2858"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efced153b3294552bf69fb0952b844e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7f9b37aad0f400cad7f6989c7841e67"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 28:17, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy Head1</th>\n      <th>Accuracy Head2</th>\n      <th>Overall Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.547500</td>\n      <td>0.536553</td>\n      <td>0.722500</td>\n      <td>0.782500</td>\n      <td>0.752500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.488900</td>\n      <td>0.535974</td>\n      <td>0.755000</td>\n      <td>0.763750</td>\n      <td>0.759375</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.463300</td>\n      <td>0.525728</td>\n      <td>0.752500</td>\n      <td>0.793750</td>\n      <td>0.773125</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.426200</td>\n      <td>0.535208</td>\n      <td>0.765000</td>\n      <td>0.792500</td>\n      <td>0.778750</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.407900</td>\n      <td>0.536547</td>\n      <td>0.767500</td>\n      <td>0.797500</td>\n      <td>0.782500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nimport torch.nn as nn\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nfrom safetensors.torch import load_file\n\nclass RobertaWithTwoHeads(nn.Module):\n    def __init__(self, base_model, num_labels_head1=2, num_labels_head2=2, hidden_size=768):\n        super(RobertaWithTwoHeads, self).__init__()\n        self.roberta = base_model.roberta\n        self.dropout = nn.Dropout(base_model.config.hidden_dropout_prob)\n        self.config = base_model.config\n        \n        # Enhanced classification head 1 for hate detection\n        self.head1 = nn.Sequential(\n            nn.Linear(self.config.hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.GELU(),\n            nn.Dropout(0.2),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.LayerNorm(hidden_size // 2),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size // 2, num_labels_head1)\n        )\n        \n        # Enhanced classification head 2 for fake news detection\n        self.head2 = nn.Sequential(\n            nn.Linear(self.config.hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.GELU(),\n            nn.Dropout(0.2),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.LayerNorm(hidden_size // 2),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size // 2, num_labels_head2)\n        )\n        \n        # Optional: Add residual connections\n        self.use_residual = False\n        if self.use_residual:\n            self.residual_projection1 = nn.Linear(self.config.hidden_size, num_labels_head1)\n            self.residual_projection2 = nn.Linear(self.config.hidden_size, num_labels_head2)\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        labels=None,\n        return_dict=None,\n        **kwargs\n    ):\n        outputs = self.roberta(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            return_dict=True\n        )\n\n        sequence_output = outputs.last_hidden_state\n        pooled_output = sequence_output[:, 0, :]  # [CLS] token\n        pooled_output = self.dropout(pooled_output)\n\n        # Forward pass through enhanced heads\n        head1_output = self.head1(pooled_output)\n        head2_output = self.head2(pooled_output)\n\n        # Add residual connections if enabled\n        if self.use_residual:\n            residual1 = self.residual_projection1(pooled_output)\n            residual2 = self.residual_projection2(pooled_output)\n            logits_head1 = head1_output + residual1\n            logits_head2 = head2_output + residual2\n        else:\n            logits_head1 = head1_output\n            logits_head2 = head2_output\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            if labels.dim() == 2:\n                loss_head1 = loss_fct(logits_head1, labels[:, 0])\n                loss_head2 = loss_fct(logits_head2, labels[:, 1])\n                loss = (loss_head1 + loss_head2) / 2\n            else:\n                print(f\"Unexpected label shape: {labels.shape}\")\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=(logits_head1, logits_head2),\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n\ndef clean(text):\n    # Add your text cleaning function here\n    # This is a placeholder - use your actual cleaning logic\n    return str(text).strip()\n\n# Load base model and create the dual-head model\nbase_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\nmodel = RobertaWithTwoHeads(base_model)\n\n# Load the fine-tuned weights from safetensors format\nstate_dict = load_file(\"roberta-dual-head-finetuned/model.safetensors\")\nmodel.load_state_dict(state_dict)\nmodel.eval()  # Set to evaluation mode\n\n# Inference loop\ndf = pd.read_csv(\"/kaggle/input/icon-2024-sharedtask/Val_Task_A.csv\")\ncorrect_hate = 0\ncorrect_fake = 0\ntotal = 0\n\n# If GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nfor _, row in df.iterrows():\n    text = clean(str(row[\"Tweet\"]))\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n    \n    # Move inputs to the same device as model\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    # Perform inference\n    with torch.no_grad():\n        outputs = model(**inputs)\n        \n    # Extract predictions\n    logits_head1, logits_head2 = outputs.logits\n    \n    # Get predicted classes\n    pred_head1 = torch.argmax(logits_head1, dim=1)\n    pred_head2 = torch.argmax(logits_head2, dim=1)\n    \n    # Move predictions to CPU for comparison with pandas data\n    pred_head1 = pred_head1.cpu()\n    pred_head2 = pred_head2.cpu()\n    \n    # Get ground truth labels\n    true_hate = int(row[\"Hate\"])\n    true_fake = int(row[\"Fake\"])\n    \n    # Update counters\n    if pred_head1.item() == true_hate:\n        correct_hate += 1\n    if pred_head2.item() == true_fake:\n        correct_fake += 1\n    \n    total += 1\n\n# Calculate accuracies\nhate_accuracy = correct_hate / total\nfake_accuracy = correct_fake / total\noverall_accuracy = (hate_accuracy + fake_accuracy) / 2\n\nprint(f\"Hate Detection Accuracy: {hate_accuracy:.4f}\")\nprint(f\"Fake News Detection Accuracy: {fake_accuracy:.4f}\")\nprint(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n\n# Optional: Save predictions to CSV\npredictions = []\nmodel.eval()\nwith torch.no_grad():\n    for _, row in df.iterrows():\n        text = clean(str(row[\"Tweet\"]))\n        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        \n        outputs = model(**inputs)\n        logits_head1, logits_head2 = outputs.logits\n        \n        pred_hate = torch.argmax(logits_head1, dim=1).cpu().item()\n        pred_fake = torch.argmax(logits_head2, dim=1).cpu().item()\n        \n        predictions.append({\n            'Tweet': row[\"Tweet\"],\n            'Predicted_Hate': pred_hate,\n            'Predicted_Fake': pred_fake,\n            'True_Hate': row[\"Hate\"],\n            'True_Fake': row[\"Fake\"]\n        })\n\npredictions_df = pd.DataFrame(predictions)\npredictions_df.to_csv(\"predictions.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-20T09:08:02.138722Z","iopub.execute_input":"2024-11-20T09:08:02.139232Z","iopub.status.idle":"2024-11-20T09:08:16.868376Z","shell.execute_reply.started":"2024-11-20T09:08:02.139191Z","shell.execute_reply":"2024-11-20T09:08:16.867719Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Hate Detection Accuracy: 0.7562\nFake News Detection Accuracy: 0.7562\nOverall Accuracy: 0.7562\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nimport torch.nn as nn\nfrom transformers.modeling_outputs import SequenceClassifierOutput\n\nclass RobertaWithTwoHeads(nn.Module):\n    def __init__(self, base_model, num_labels_head1=2, num_labels_head2=2, hidden_size=768):\n        super(RobertaWithTwoHeads, self).__init__()\n        self.roberta = base_model.roberta\n        self.dropout = nn.Dropout(base_model.config.hidden_dropout_prob)\n        self.config = base_model.config\n        \n        # Enhanced classification head 1 for hate detection\n        self.head1 = nn.Sequential(\n            nn.Linear(self.config.hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.GELU(),\n            nn.Dropout(0.2),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.LayerNorm(hidden_size // 2),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size // 2, num_labels_head1)\n        )\n        \n        # Enhanced classification head 2 for fake news detection\n        self.head2 = nn.Sequential(\n            nn.Linear(self.config.hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.GELU(),\n            nn.Dropout(0.2),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.LayerNorm(hidden_size // 2),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size // 2, num_labels_head2)\n        )\n        \n        # Optional: Add residual connections\n        self.use_residual = False\n        if self.use_residual:\n            self.residual_projection1 = nn.Linear(self.config.hidden_size, num_labels_head1)\n            self.residual_projection2 = nn.Linear(self.config.hidden_size, num_labels_head2)\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        labels=None,\n        return_dict=None,\n        **kwargs\n    ):\n        outputs = self.roberta(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            return_dict=True\n        )\n\n        sequence_output = outputs.last_hidden_state\n        pooled_output = sequence_output[:, 0, :]  # [CLS] token\n        pooled_output = self.dropout(pooled_output)\n\n        # Forward pass through enhanced heads\n        head1_output = self.head1(pooled_output)\n        head2_output = self.head2(pooled_output)\n\n        # Add residual connections if enabled\n        if self.use_residual:\n            residual1 = self.residual_projection1(pooled_output)\n            residual2 = self.residual_projection2(pooled_output)\n            logits_head1 = head1_output + residual1\n            logits_head2 = head2_output + residual2\n        else:\n            logits_head1 = head1_output\n            logits_head2 = head2_output\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            if labels.dim() == 2:\n                loss_head1 = loss_fct(logits_head1, labels[:, 0])\n                loss_head2 = loss_fct(logits_head2, labels[:, 1])\n                loss = (loss_head1 + loss_head2) / 2\n            else:\n                print(f\"Unexpected label shape: {labels.shape}\")\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=(logits_head1, logits_head2),\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n\ndef clean(text):\n    # Add your text cleaning function here\n    # This is a placeholder - use your actual cleaning logic\n    return str(text).strip()\n\n# Load base model and create the dual-head model\nbase_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\nmodel = RobertaWithTwoHeads(base_model)\n\n# Load the fine-tuned weights from safetensors format\nstate_dict = load_file(\"roberta-dual-head-finetuned/model.safetensors\")\nmodel.load_state_dict(state_dict)\nmodel.eval()  # Set to evaluation mode\n\n# Inference loop\ndf = pd.read_csv(\"/kaggle/input/icon-2024-sharedtask/Test_Task_A.csv\")\n\n# If GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\npredictions = []\nmodel.eval()\nwith torch.no_grad():\n    for _, row in df.iterrows():\n        text = clean(str(row[\"Tweet\"]))\n        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        \n        outputs = model(**inputs)\n        logits_head1, logits_head2 = outputs.logits\n        \n        pred_hate = torch.argmax(logits_head1, dim=1).cpu().item()\n        pred_fake = torch.argmax(logits_head2, dim=1).cpu().item()\n        \n        predictions.append({\n            'Id': row[\"Id\"],\n            'Tweet': row[\"Tweet\"],\n            'Hate': pred_hate,\n            'Fake': pred_fake\n        })\n\npredictions_df = pd.DataFrame(predictions)\npredictions_df.to_csv(\"KeyboardWarriors_TaskA_run2.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-20T09:08:16.869789Z","iopub.execute_input":"2024-11-20T09:08:16.870266Z","iopub.status.idle":"2024-11-20T09:08:24.767854Z","shell.execute_reply.started":"2024-11-20T09:08:16.870227Z","shell.execute_reply":"2024-11-20T09:08:24.766840Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"print(\"Done\")","metadata":{"execution":{"iopub.status.busy":"2024-11-20T09:08:24.769057Z","iopub.execute_input":"2024-11-20T09:08:24.769320Z","iopub.status.idle":"2024-11-20T09:08:24.773779Z","shell.execute_reply.started":"2024-11-20T09:08:24.769295Z","shell.execute_reply":"2024-11-20T09:08:24.772791Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Done\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}